{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron Multicapa.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6s_tDWBwn5q6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "059b702f-7999-4657-bfba-e29c9d8757c9"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.nn import softmax, relu, relu6, sigmoid, tanh\n",
        "import numpy as np\n",
        "\n",
        "# Variables\n",
        "\n",
        "TRAIN_SET_SIZE = 60000 #DO NOT CHANGE\n",
        "TEST_SET_SIZE = 10000 #DO NOT CHANGE\n",
        "NORMALIZATION = 255.0 #DO NOT CHANGE\n",
        "IMAGE_LENGTH = 28 #Length of MNIST image side DO NOT CHANGE\n",
        "\n",
        "INPUT = 10 #Size of input array, MINIMUM 10\n",
        "HIDDEN_NEURONS1 = 2.5*(28*28) #Neurons in hidden layer 1\n",
        "HIDDEN_NEURONS2 = 2.5*(28*28)\n",
        "HIDDEN_NEURONS3 = 2.5*(28*28)\n",
        "HIDDEN_NEURONS4 = 2.5*(28*28)\n",
        "HIDDEN_NEURONS5 = 2.5*(28*28)\n",
        "\n",
        "HIDDEN_LAYERS = 1\n",
        "EPOCHS = 1 #Number of times network will be exposed to training set\n",
        "LEARNING_RATE = 0.001 #Default is 0.001\n",
        "DROPOUT = 0.2\n",
        "\n",
        "KERNEL_IN='random_uniform' #Initializer for the layer weights\n",
        "BIAS_IN='ones' #Initializer for the bias\n",
        "ACTIVATION1=relu #Activation function for hidden layer 1, should be non-linear for hidden layers in multilayer perceptrons\n",
        "ACTIVATION2=relu\n",
        "ACTIVATION3=relu\n",
        "ACTIVATION4=relu\n",
        "ACTIVATION5=relu\n",
        "\n",
        "# Uncomment the desired functions, only one of a kind should be uncommented at the same time\n",
        "#MODEL_OPTIMIZER=SGD(lr=LEARNING_RATE) #Technique used to upgrade network weights\n",
        "#MODEL_OPTIMIZER=Adam(lr=LEARNING_RATE)\n",
        "MODEL_OPTIMIZER=RMSprop(lr=LEARNING_RATE)\n",
        "#LOSS='mse' #Loss function\n",
        "#LOSS='binary_crossentropy'\n",
        "#LOSS='categorical_crossentropy'\n",
        "LOSS='sparse_categorical_crossentropy'\n",
        "METRICS=['accuracy']\n",
        "\n",
        "#MNIST database and normalization\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / NORMALIZATION, x_test / NORMALIZATION\n",
        "\n",
        "# Layers\n",
        "\n",
        "HiddenLayer1 = Dense(HIDDEN_NEURONS1, kernel_initializer=KERNEL_IN, bias_initializer=BIAS_IN, activation=ACTIVATION1, input_dim=INPUT) #Layer will have <HIDDEN_NEURONS1> neurons and an input array with size <INPUT>\n",
        "HiddenLayer2 = Dense(HIDDEN_NEURONS2, kernel_initializer=KERNEL_IN, bias_initializer=BIAS_IN, activation=ACTIVATION2) #Size of input array will be number of neurons of previous layer\n",
        "HiddenLayer3 = Dense(HIDDEN_NEURONS3, kernel_initializer=KERNEL_IN, bias_initializer=BIAS_IN, activation=ACTIVATION3)\n",
        "HiddenLayer4 = Dense(HIDDEN_NEURONS4, kernel_initializer=KERNEL_IN, bias_initializer=BIAS_IN, activation=ACTIVATION4)\n",
        "HiddenLayer5 = Dense(HIDDEN_NEURONS5, kernel_initializer=KERNEL_IN, bias_initializer=BIAS_IN, activation=ACTIVATION5)\n",
        "OutputLayer = Dense(10, kernel_initializer=KERNEL_IN, bias_initializer=BIAS_IN, activation=softmax) #Activation function should be softmax, number of neurons should be 10\n",
        "\n",
        "# Model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten()) #Flattens input matrix into 1-dimension vector\n",
        "model.add(HiddenLayer1)\n",
        "model.add(Dropout(DROPOUT))\n",
        "if HIDDEN_LAYERS > 1:\n",
        "  model.add(HiddenLayer2)\n",
        "  model.add(Dropout(DROPOUT))\n",
        "if HIDDEN_LAYERS > 2:\n",
        "  model.add(HiddenLayer3)\n",
        "  model.add(Dropout(DROPOUT))\n",
        "if HIDDEN_LAYERS > 3:\n",
        "  model.add(HiddenLayer4)\n",
        "  model.add(Dropout(DROPOUT))\n",
        "if HIDDEN_LAYERS > 4:\n",
        "  model.add(HiddenLayer5)\n",
        "  model.add(Dropout(DROPOUT))\n",
        "model.add(OutputLayer)\n",
        "\n",
        "# Compilation\n",
        "\n",
        "model.compile(optimizer=MODEL_OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
        "\n",
        "# Training\n",
        "\n",
        "model.fit(x_train, y_train, epochs=EPOCHS)\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "#evaluationVector = model.evaluate(x_test, y_test)\n",
        "evaluationVector = model.evaluate(x_train, y_train)\n",
        "\n",
        "#print(\"\\nError rate over test set was: \", (1-evaluationVector[1])*100, \"%\")\n",
        "print(\"\\nError rate over training set was: \", (1-evaluationVector[1])*100, \"%\")\n",
        "\n",
        "# Label printing - Can be entirely commented for faster execution\n",
        "\n",
        "predictedLabels = model.predict(x_train.reshape((-1,28*28)))\n",
        "#predictedLabels = model.predict(x_test.reshape((-1,28*28))) #x_test is reshaped from 1-D to a 2-D array where all elements - 10000 - are arrays with 28*28 elements, that is, one MNIST digit\n",
        "#Value -1 is used to ensure total size of array is kept constant - Original array has 784000 elements and one of its dimensions will have length 28*28, so other dimension must have length 10000\n",
        "'''\n",
        "prediction = ''\n",
        "for i in range(TEST_SET_SIZE):\n",
        " prediction=prediction+str(np.argmax(predictedLabels, axis=1)[i]) #Each element in predictedLabels will be array with 10 floats, maximum value from those must be picked with np.argmax\n",
        "print(\"\\nThe network predicted the following labels for the test set:\")\n",
        "print (prediction)\n",
        "\n",
        "actual = ''\n",
        "for i in range(TEST_SET_SIZE):\n",
        "  actual=actual+str(y_test[i])\n",
        "print(\"\\nActual test set labels are:\")\n",
        "print(actual)'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 10s 159us/step - loss: 0.4181 - acc: 0.9181\n",
            "60000/60000 [==============================] - 4s 66us/step\n",
            "\n",
            "Error rate over training set was:  2.7583333333333293 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprediction = \\'\\'\\nfor i in range(TEST_SET_SIZE):\\n prediction=prediction+str(np.argmax(predictedLabels, axis=1)[i]) #Each element in predictedLabels will be array with 10 floats, maximum value from those must be picked with np.argmax\\nprint(\"\\nThe network predicted the following labels for the test set:\")\\nprint (prediction)\\n\\nactual = \\'\\'\\nfor i in range(TEST_SET_SIZE):\\n  actual=actual+str(y_test[i])\\nprint(\"\\nActual test set labels are:\")\\nprint(actual)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    }
  ]
}